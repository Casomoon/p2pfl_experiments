import torch 
import random
import sys
import pandas as pd
import pytorch_lightning as pl 
from copy import deepcopy
from torch.utils.data import Dataset, DataLoader, random_split, Subset
from transformers import BertTokenizerFast
from logging import Logger 
from pathlib import Path
from collections import defaultdict, Counter
from p2pfl.management.logger import logger

# class loads the dataset and creates the distributed data collections for each client
class NLIParser(): 
    def __init__(
                self,  
                data_loc: Path, 
                num_clients: int,  
                data_dist_weights: list[float],
                batch_size:int =1, 
                shuffle: bool = True):
        self.module_name = "NLIParser"
        if not data_loc.exists() : 
            logger.error(self.module_name, f"{data_loc.absolute} not found.")
            raise FileNotFoundError(f"{data_loc.absolute} not found.") 
        assert sum(data_dist_weights) == 1.0
        assert len(data_dist_weights) == num_clients
        self.num_clients = num_clients
        self.data_dist_weights = data_dist_weights
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.data_loc = data_loc
        self.files : dict= {
            "train"             : {"filename" : "multinli_1.0_train.jsonl"},
            "test_matched"      : {"filename" : "multinli_1.0_dev_matched.jsonl"},
            "test_mismatched"   : {"filename" : "multinli_1.0_dev_mismatched.jsonl"},
        }   
        self.columns_to_keep = ["pairID", "genre", "gold_label", "sentence1", "sentence2"]
        self.__load_datasets()
        self.__nli_to_cd()
    
    def __load_datasets(self): 
        for set_name, set_dict in self.files.items(): 
            logger.info(self.module_name, f"Loading set {set_name}")
            filepath = self.data_loc.joinpath(set_dict.get("filename"))
            dataset = pd.read_json(filepath, lines=True)   
            dataset = dataset[self.columns_to_keep]
            self.files[set_name]["frame"] = dataset
            logger.info(self.module_name, f"{set_name} loaded, shape is: {dataset.shape}")
        logger.info(self.module_name, "Datasets have been loaded successfully.")
    
    def __nli_to_cd(self):
        # replaces all neutral  
        for dataset in self.files.keys():
            logger.info(self.module_name, dataset)
            frame : pd.DataFrame = self.files.get(dataset).get("frame")
            frame_edited = deepcopy(frame)
            frame_edited["gold_label"] = frame["gold_label"].apply(
                lambda x : 
                "no_contradiction" if (x == "entailment" or x == "neutral")
                else x
            ) 
            self.files[dataset]["frame"] = frame_edited

    
    #def get_iid_split(self, num_clients, data_dist_weights: list[float], batch_size = 1, shuffle = True) -> list[DataLoader]:
    #    assert sum(data_dist_weights) == 1.0
    #    assert len(data_dist_weights) == num_clients
    #    
    #    train_frame = deepcopy(self.files["train"]["frame"])
    #    total_samples = len(train_frame)
    #    client_sample_counts = [int(total_samples*weight) for weight in data_dist_weights]


    def get_non_iid_split(self, ) -> list[DataLoader]:
        """
        Generates train/test dataloaders for training derived from the large train dataset. Also generates a global test set
        which is generated by combining test_matched and test_mismatched to one big frame with then 5% data of the train/test set.
        """
        if  hasattr(self, "train_loaders") and \
            hasattr(self, "val_loaders") and \
            hasattr(self, "global_test_set"):
            return self.train_loaders, self.val_loaders, self.global_test_set
        
        # setup and combination of the matched and mismatched for global valdiation dataset 
        train_frame = deepcopy(self.files["train"]["frame"])
        total_samples = len(train_frame)
        client_sample_counts = [int(total_samples*weight) for weight in self.data_dist_weights]
        test_matched_frame = deepcopy(self.files["test_matched"]["frame"])
        test_mismatched_frame = deepcopy(self.files["test_mismatched"]["frame"])
        global_test_set = pd.concat([test_matched_frame, test_mismatched_frame]).reset_index(drop=True)
        client_datasets, client_distributions = self.niid_split_data(train_frame, self.num_clients, client_sample_counts)
        # Log information about the data distributions of the clients.
        for i,client in enumerate(client_distributions):
            logger.info(self.module_name,f"{i}: {client}") 
       # Log the total number of samples assigned to each client
        for client_idx, dataset in enumerate(client_datasets):
            logger.info(self.module_name, f"Total samples assigned to client {client_idx}: {len(dataset)}")
            logger.info(self.module_name, f"{type(dataset)} {type(dataset[0])}")
       # Create train and test split for each client, they are returned as list of tuples of dataloaders then
        train_loaders, val_loaders = self.train_test_split(client_datasets, 
                                                          validation_split = 0.2, 
                                                          batch_size = self.batch_size, 
                                                          train_frame=train_frame, 
                                                          shuffle = True)
        self.train_loaders = train_loaders
        self.val_loaders = val_loaders
        self.global_test_set = global_test_set
        logger.info("Data split completed successfully. Assigning to instance variables.")
        
        return train_loaders, val_loaders, global_test_set
    
    def train_test_split(self, client_datasets : list[list[int]], validation_split : float, batch_size : int, train_frame : pd.DataFrame, shuffle = True)-> tuple[list[DataLoader], list[DataLoader]]: 
        train_loaders: list[DataLoader] = []
        val_loaders: list[DataLoader]= []
        for cid, client_dataset in enumerate(client_datasets): 
            # do the split for each created client dataset
            val_size = int(len(client_dataset) * validation_split)
            train_size = len(client_dataset) - val_size
            train_indices, val_indices = random_split(client_dataset, [train_size, val_size])
            train_data_subset = train_indices.indices
            val_data_subset = val_indices.indices
            logger.info(self.module_name, f"{train_indices}, {val_indices}")
            train_data = train_frame.iloc[train_data_subset]
            val_data = train_frame.iloc[val_data_subset]
            logger.info(self.module_name, len(train_data))
            logger.info(self.module_name, len(val_data))
            train_subset = NLIDataset(cid, train_data, train=True)
            val_subset = NLIDataset(cid, val_data, train=True)
            train_loaders.append(DataLoader(train_subset, batch_size, shuffle = shuffle))
            val_loaders.append(DataLoader(val_subset, batch_size=batch_size, shuffle = shuffle))
        return train_loaders, val_loaders


    def niid_split_data(self, frame, num_clients, client_sample_counts : list[int]):
        genres_dict = defaultdict(list)
        for idx, row in frame.iterrows():
            genres_dict[(row["genre"])].append(idx)
        client_datasets = [[] for _ in range(num_clients)]
        client_distributions = [Counter() for _ in range(num_clients)]
        logger.info(self.module_name, "Starting data split...")
        genres = list(genres_dict.items())
        random.shuffle(genres)  # Shuffle genre/label pairs for random assignment
        client_current_counts = [0] * num_clients
        # Assign full genre/label combinations to clients
        for genre, indices in genres:
            assigned = False 
            # Try to find a client that can accept the whole dataset
            client_idx = min(range(num_clients), key = lambda i : 
                             (client_current_counts[i] + len(indices)> client_sample_counts[i],
                              client_current_counts[i])) 
            logger.info(self.module_name, f"{client_idx} has been chosen for {genre} assignment.")
            if client_current_counts[client_idx] + len(indices)<= client_sample_counts[client_idx]:
                client_datasets[client_idx].extend(indices)
                client_distributions[client_idx].update([genre] * len(indices))
                client_current_counts[client_idx] += len(indices)
                assigned = True
                logger.info(self.module_name, f"Assigned all {len(indices)} samples of {genre} to client {client_idx}.")
            if not assigned:
                remaining_indices = indices 
                while remaining_indices: 
                    client_idx = min(range(num_clients),
                                     key = lambda i: 
                                     (client_current_counts[i]>=client_sample_counts[i],
                                     client_current_counts[i]))
                    space_left = client_sample_counts[client_idx]- client_current_counts[client_idx]
                    if space_left>0:
                        split_indices = remaining_indices[:space_left]
                        client_datasets[client_idx].extend(split_indices)
                        client_distributions[client_idx].update([genre] * len(split_indices))
                        client_current_counts[client_idx] += len(split_indices)
                        remaining_indices = remaining_indices[space_left:]
                    else : break
        for client_idx, dataset in enumerate(client_datasets):
            logger.info(self.module_name, f"Total samples assigned to client {client_idx}: {len(dataset)}")
        return client_datasets, client_distributions
    
class NLIDataModule(pl.LightningDataModule):
    def __init__(
                self, 
                parser: NLIParser,
                cid: int, 
                niid: bool = True
                ):
        super().__init__()
        self.parser = parser
        self.cid = cid
        self.niid = niid
        self.train_loaders: list[DataLoader]
        self.val_loaders: list[DataLoader]
        self.global_test: DataLoader
        split_dataset = self.parser.get_non_iid_split()
        self.train_loaders, self.val_loaders, self.global_test = split_dataset
    
    def setup(self, stage: str = None):
        """
        Called at the beginning of the fit, test, or predict process.
        Loads the data splits into train, val, and test dataloaders.
        """
        # Perform the non-IID split and get the corresponding DataLoader objects
        
    
    def train_dataloader(self):
        """
        Returns the training dataloaders for each client as a list.
        """
        return self.train_loaders[self.cid]

    def val_dataloader(self):
        """
        Returns the validation dataloaders for each client as a list.
        """
        return self.val_loaders[self.cid]

    def test_dataloader(self):
        """
        Returns the global test dataloader.
        """
        return self.global_test
    

    
class NLIDataset(Dataset): 
    def __init__(self, cid: int, df : pd.DataFrame, train : bool = False) -> None:
        self.cid = cid 
        self.module_name = f"NLIDataset {self.cid}"
        self.training = train
        # tokenizer
        self.tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
        self.keys_encoding = ["input_ids", "token_type_ids", "attention_mask"]
        # label map directly translates to 0 -> no contradiction, 1 -> contradiction
        self.data = deepcopy(df) 
        self.label_map = {"no_contradiction": 0, "contradiction": 1}
        # preprocess with tokenizer
        logger.info(self.module_name, f"Preprocessing sentences for Client {self.cid} with length {len(self.data)}.")
        self.data["encoded"] = self.data.apply(
            lambda row : self.tokenizer(
                row["sentence1"], row["sentence2"], truncation = True,
                return_tensors = "pt", padding = "max_length"), axis = 1)
        if self.training : 
            logger.info(self.module_name, "Preprocessing labels for training loader")
            self.data["label"] = self.data["gold_label"].apply(
                lambda label : self.label_map.get(label))
            
    def __len__(self) -> int: 
        return len(self.data)
    
    def __getitem__(self, index) -> dict:
        row = self.data.iloc[index]
        encoding = row["encoded"]
        input = {k : v.squeeze(0) for k,v in encoding.items()}
        if not self.training : return input
        cd_label : int = row["label"]
        input["label"] = torch.tensor(cd_label).long()
        return input
        
    
