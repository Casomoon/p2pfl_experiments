import random
import pandas as pd
import gc
import math 
import sys
from transformers import BertTokenizerFast
from copy import deepcopy
from torch.utils.data import DataLoader, random_split
from pathlib import Path
from collections import defaultdict, Counter
from p2pfl.management.logger import logger

from .nli_pl_wrapper import NLIDataset, NLIDataModule



tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
label_map = {"no_contradiction": 0, "contradiction": 1}

# class loads the dataset and creates the distributed data collections for each client
class NLIParser(): 
    def __init__(
                self,  
                data_loc: Path, 
                num_clients: int,  
                data_dist_weights: list[float],
                batch_size:int =1, 
                validation_split = 0.2, 
                shuffle: bool = True):
        self.module_name = "NLIParser"
        if not data_loc.exists() : 
            logger.error(self.module_name, f"{data_loc.absolute} not found.")
            raise FileNotFoundError(f"{data_loc.absolute} not found.") 
        assert math.isclose(sum(data_dist_weights), 1.0)
        assert len(data_dist_weights) == num_clients
        self.num_clients = num_clients
        self.data_dist_weights = data_dist_weights
        self.batch_size = batch_size
        self.validation_split = validation_split
        self.shuffle = shuffle
        self.data_loc = data_loc
        self.files : dict= {
            "train"             : {"filename" : "multinli_1.0_train.jsonl"},
            "test_matched"      : {"filename" : "multinli_1.0_dev_matched.jsonl"},
            "test_mismatched"   : {"filename" : "multinli_1.0_dev_mismatched.jsonl"},
        }   
        self.columns_to_keep = ["pairID", "genre", "gold_label", "sentence1", "sentence2"]
        self.__load_datasets()
        self.__nli_to_cd()
    
    def __load_datasets(self): 
        for set_name, set_dict in self.files.items(): 
            logger.info(self.module_name, f"Loading set {set_name}")
            filepath = self.data_loc.joinpath(set_dict.get("filename"))
            dataset = pd.read_json(filepath, lines=True)   
            logger.info(self.module_name, f"Set {set_name} has a length of {len(dataset)}")
            dataset = dataset[self.columns_to_keep]
            self.files[set_name]["frame"] = dataset
            logger.info(self.module_name, f"{set_name} loaded, shape is: {dataset.shape}")
        logger.info(self.module_name, "Datasets have been loaded successfully.")
    
    def __nli_to_cd(self):
        # replaces all neutral  
        for dataset in self.files.keys():
            logger.info(self.module_name, dataset)
            frame : pd.DataFrame = self.files.get(dataset).get("frame")
            frame_edited = deepcopy(frame)
            frame_edited["gold_label"] = frame["gold_label"].apply(
                lambda x : 
                "no_contradiction" if (x == "entailment" or x == "neutral")
                else x
            ) 
            self.files[dataset]["frame"] = frame_edited

    def get_non_iid_split(self, ) -> list[NLIDataModule]:
        """
        Generates train/test dataloaders for training derived from the large train dataset. Also generates a global test set
        which is generated by combining test_matched and test_mismatched to one big frame with then 5% data of the train/test set.
        """
        # Only do the split generation once, even if the method is called multiple times
        if  hasattr(self, "nli_data_modules") : 
            return self.nli_data_modules
        
        # setup and combination of the matched and mismatched for global valdiation dataset 
        train_frame = deepcopy(self.files["train"]["frame"])
        total_samples = len(train_frame)
        client_sample_counts = [int(total_samples*weight) for weight in self.data_dist_weights]
        test_matched_frame = deepcopy(self.files["test_matched"]["frame"])
        test_mismatched_frame = deepcopy(self.files["test_mismatched"]["frame"])
        global_test_df = pd.concat([test_matched_frame, test_mismatched_frame]).reset_index(drop=True)
        len_bef_drop = len(global_test_df)
        logger.info(self.module_name, f"Test frame has {len_bef_drop} rows.")
        # throw out the trash causing the index error during training 
        global_test_df = global_test_df.loc[global_test_df["gold_label"] != "-"]
        dropped = len_bef_drop - len(global_test_df)
        logger.info(self.module_name, f"Dropped {dropped} rows on global test frame with the gold_label '-'." )
        client_datasets, client_distributions = self.niid_split_data(train_frame, self.num_clients, client_sample_counts)
        # Log information about the data distributions of the clients.
        for i,client in enumerate(client_distributions):
            logger.info(self.module_name,f"{i}: {client}") 
       # Log the total number of samples assigned to each client
        for client_idx, dataset in enumerate(client_datasets):
            logger.info(self.module_name, f"Total samples assigned to client {client_idx}: {len(dataset)}")
       # Create train and test split for each client, they are returned as list of tuples of dataloaders then
        train_dfs, val_dfs = self.train_test_split( client_datasets, 
                                                    validation_split = self.validation_split, 
                                                    train_frame=train_frame
                                                    )
    
        assert len(train_dfs) == len(val_dfs)
        # encode all datasets 
        trains_encoded, vals_encoded, global_test_encoded = self.encode(train_dfs, val_dfs, global_test_df)
        # transform to pl data modules 
        nli_data_modules = self.to_data_modules(trains_encoded, vals_encoded, global_test_encoded, self.batch_size)
        self.nli_data_modules = nli_data_modules
        return self.nli_data_modules

        
       
    
    def niid_split_data(self, frame, num_clients, client_sample_counts : list[int]):
        genres_dict = defaultdict(list)
        for idx, row in frame.iterrows():
            genres_dict[(row["genre"])].append(idx)
        client_datasets = [[] for _ in range(num_clients)]
        client_distributions = [Counter() for _ in range(num_clients)]
        logger.info(self.module_name, "Starting data split...")
        genres = list(genres_dict.items())
        random.shuffle(genres)  # Shuffle genre/label pairs for random assignment
        client_current_counts = [0] * num_clients
        # Assign full genre/label combinations to clients
        for genre, indices in genres:
            assigned = False 
            # Try to find a client that can accept the whole dataset
            client_idx = min(range(num_clients), key = lambda i : 
                             (client_current_counts[i] + len(indices)> client_sample_counts[i],
                              client_current_counts[i])) 
            logger.info(self.module_name, f"{client_idx} has been chosen for {genre} assignment.")
            if client_current_counts[client_idx] + len(indices)<= client_sample_counts[client_idx]:
                client_datasets[client_idx].extend(indices)
                client_distributions[client_idx].update([genre] * len(indices))
                client_current_counts[client_idx] += len(indices)
                assigned = True
                logger.info(self.module_name, f"Assigned all {len(indices)} samples of {genre} to client {client_idx}.")
            if not assigned:
                remaining_indices = indices 
                while remaining_indices: 
                    client_idx = min(range(num_clients),
                                     key = lambda i: 
                                     (client_current_counts[i]>=client_sample_counts[i],
                                     client_current_counts[i]))
                    space_left = client_sample_counts[client_idx]- client_current_counts[client_idx]
                    if space_left>0:
                        split_indices = remaining_indices[:space_left]
                        client_datasets[client_idx].extend(split_indices)
                        client_distributions[client_idx].update([genre] * len(split_indices))
                        client_current_counts[client_idx] += len(split_indices)
                        remaining_indices = remaining_indices[space_left:]
                    else : break
        for client_idx, dataset in enumerate(client_datasets):
            logger.info(self.module_name, f"Total samples assigned to client {client_idx}: {len(dataset)}")
        return client_datasets, client_distributions
    
    def train_test_split(self, client_datasets : list[list[int]], validation_split : float, train_frame : pd.DataFrame)-> tuple[list[pd.DataFrame], list[pd.DataFrame]]: 
        train_dfs: list[pd.DataFrame] = []
        val_dfs: list[pd.DataFrame]= []
        for client_dataset in client_datasets: 
            # do the split for each created client dataset
            val_size = int(len(client_dataset) * validation_split)
            train_size = len(client_dataset) - val_size
            train_indices, val_indices = random_split(client_dataset, [train_size, val_size])
            train_data_subset = train_indices.indices
            val_data_subset = val_indices.indices
            train_data = train_frame.iloc[train_data_subset]
            val_data = train_frame.iloc[val_data_subset]
            logger.info(self.module_name,f"Types : {type(train_data)}, {type(val_data)}")
            train_dfs.append(train_data)
            val_dfs.append(val_data)
        return train_dfs, val_dfs
    
    
    def encode(self, train_data_dfs: list[pd.DataFrame], val_data_dfs: list[pd.DataFrame], global_test_data: pd.DataFrame)-> tuple[list[list[dict]], list[list[dict]], list[dict]]: 
        assert len(train_data_dfs) == len(val_data_dfs)
        label_map = {"no_contradiction": 0, "contradiction": 1}
        batch_tokenize = 1000
        def process_data_frames(data_frames: list[pd.DataFrame]) -> list[list[dict]]:
            data_dicts: list[list[dict]] = []
            for i, df in enumerate(data_frames):
                df = df.copy()
                logger.info(self.module_name, f"Encoding sentence pairs for client {i}")
                df.loc[:, "encoded"] = self.batch_tokenize(df, batch_tokenize)
                df.loc[:, "label"] = df["gold_label"].apply(lambda label: label_map.get(label))
                encoded = df.to_dict(orient="records")
                data_dicts.append(encoded)
                del df
                gc.collect()
            return data_dicts
        logger.info(self.module_name, "Processing train frames.")
        train_dicts = process_data_frames(train_data_dfs)
        logger.info(self.module_name, "Processing val frames.")
        val_dicts = process_data_frames(val_data_dfs)
         # Process global test data
        logger.info(self.module_name, "Encoding global test data")
        global_test_data.loc[:,"encoded"] = self.batch_tokenize(global_test_data, batch_tokenize)
        global_test_data.loc[:, "label"] = global_test_data["gold_label"].apply(
            lambda label: label_map.get(label)
        )
        test_dicts = global_test_data.to_dict(orient="records")
        del global_test_data
        gc.collect()
            
        return train_dicts, val_dicts, test_dicts
    
    def batch_tokenize(self, df, batch_size=1000):
        encoded_data: list = []
        num_batches = len(df) // batch_size + (1 if len(df) % batch_size != 0 else 0)
        
        for i in range(num_batches):
            # Slice the dataframe into batches
            batch = df.iloc[i * batch_size: (i + 1) * batch_size]
            logger.info(self.module_name, f"Tokenizing batch {i + 1}/{num_batches}...")
            # Perform batch tokenization
            tokenized_batch = tokenizer.batch_encode_plus(
                list(zip(batch["sentence1"], batch["sentence2"])),
                truncation=True,
                padding="max_length",
                return_tensors="pt"
            )
            # Match the tokenized batch to a list structure that has the same length as the dataframe
            for index in range(len(batch)): 
                encoded_data.append({
                    "input_ids": tokenized_batch["input_ids"][index],
                    "token_type_ids": tokenized_batch["token_type_ids"][index],
                    "attention_mask": tokenized_batch["attention_mask"][index],
                })
            # Clear memory if necessary
            gc.collect()
        logger.info(self.module_name, f"Batch tokenization completed for {len(df)} samples.")
        return encoded_data
    
    def to_data_modules(self, train_sets: list[list[dict]], val_sets: list[list[dict]], test_set: list[dict], batch_size: int = 8)-> list[NLIDataModule]: 
        assert len(train_sets) == len(val_sets)
        nli_data_modules: list[NLIDataModule] = []
        global_test_loader = DataLoader(NLIDataset(cid = 500, phase="test", data = test_set, train = True), batch_size = self.batch_size, shuffle = True, num_workers = 2 )
        for i in range(len(train_sets)): 
            train_loader = DataLoader(NLIDataset(cid = i, phase = "train", data = train_sets[i], train = True), batch_size = self.batch_size, shuffle = True, num_workers = 2)
            val_loader   = DataLoader(NLIDataset(cid = i, phase = "val", data = val_sets[i], train = True), batch_size = self.batch_size, shuffle = False, num_workers = 2 )
            nli_data_modules.append(NLIDataModule(i, train_loader,val_loader,global_test_loader))        
        return nli_data_modules